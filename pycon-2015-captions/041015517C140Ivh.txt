  
>> Folks, we're going to GET started in about one minute.  If you would please take your seats.
  All right.  Welcome to the 1:40 session.  Our first speaker is talking about distributed systems.  Thank you.
  [Applause]
  >> Hi, everyone.  Welcome to your scheduled post lunch lull.  Hope y'all had a good lunch.  I'll talk about introduction to distributed systems.  It is fairly introductory if you have some experience I will not be offended if you walk out.  The slides are online I'll show the link again at the end.  Brief introduction LVH, hi.  We're PyCon I'm sure you've gathered.  A very special time of the year.  A lot of friends only get to see once a year it's at PyCon.  This year I have couple of staff positions and two talk slots.  I'm not trying to brag that's more of an apology in advantage I have been running on what I can describe as very serious sleep deficit with a little bit of luck I'm going be to coherent.  Only one way to find out.
  I work at Rackspace.  Cool space to look, look at the booth.  We're a sponsor.  That Rackspace most is on a project called autoscale which internally named otter.  Gives me an execute to put animal pictures in my slides.  I wouldn't normally mention that except that autoscale is very interesting.  It's a distributed system but also manages distributed systems.  And all of those are running on top of distributed systems, all of the systems that it orchestrates are distributed systems.  And it's doing that at fairly reasonable scale.  We see some interesting things.  Goals for today is basically just enough distributed systems to whet your appetite and shoot yourself in the foot.  These are not opposing goals, these are basically exactly the same thing.  Now, my goals today are not to be exhaustive not to be pedantically correct.  Not to say it's not important but say I don't have enough time to spend on strict mathematical correctness.  Just to give you idea of what there is, where to look, convince that you distribute systems are tricky who is paradoxically that means that the worst job I do as a presenter you think it's more complicated and there for I've succeeded and actually done better.  Cool.  Anyway, so, distribute systems, what's up with that.  Leslie has a quote that says, a system is distributed when a machine that I have never heard of can cause my program to fail.  That brings us to paradox I've talk to a lot of people -- people tell me why the almost is availability.  Reliability, availability, why it usually distributed database and reason they use that instead of something that's not distributed is because, it fails less often, allegedly.  When you listen to experts in the field like the only thing that they always mention is super special about distributed systems what makes them so hard is failure.  So, that's an interesting paradox, hopefully we'll be able to elaborate on that a little bit more during this talk.
  Now, distributed systems are really just systems that abide by two fundamental constraints.  Information travels at the speed of light.  No faster, usually slower.  And secondly components fail.  So, two very simple things and basically anything that has to do with this is probably distributed.  Sun produced a list of fallacies, these are things that many junior engineers when they start doing distributed systems that they believe.  End up invariably being false.  We're going to deal mostly with the top couple of these in this talk.  So there is the network is reliable, it's not.  It fails.  Latency isn't zero it takes awhile to get packets from one side to the things.  Band wit isn't infinite.  The network is hellavu environment.  Plenty of administrators.  Transport cost is pretty high post of the time.  And the network is anything but homogeneous.  Lots of different kinds ever clients.
  So, what is an example of distributed system?
  I could say, well, basically everything.  Which includes your laptop.  Because hey speed of light isn't infinite, CPU has to get something from RAM it's like two whole inches away or maybe one whole inch that's nano seconds -- you can see distributed systems affect even on modern desktop machines, modern servers.  But typically when someone talks about it what they mean is, any system with more than one machine and they're connected via network.  The difference between these is that networks fail far more often.  Think about it like down time for a network is measured in say, hours.  But it would not be acceptable for your ram to not work one hour out of the year.
  So, I'm going to talk about bad news mostly cheery and consequences.  Most famous theorem is the cap theorem something that everybody talks about all the time.  Talking about distributed systems.  It says that you can have any two.  You can either be consistent, be available or you can be partition tolerant, pick any two of those but not all three.  What does that mean.  Because I just said a bunch of words.  Whatever, man.  Consistently means linearized ability.  You don't understand yet explain in terms much word that you don't understand yet.  You can probably think of it as, what you expect Python to do on your local machine like when you assign stuff to names, it's like more or less linearlizability.  Availability means that all active nodes will answer any query and partition tolerance is basically resistance to failure.  So, when I say pick any two, we have this neat interest diagram that means that you probably want to be in the intersections of any two.  Except you can't be in the middle because that would mean all flee.
  Turns out that you can't actually sacrifice tolerance.  Partition tolerance is failure tolerance.  Networks and nodes fail all the time.  Latency happens, is generally more or less indistinguishable from actual failures.  Other thing is that systems fail and systems fail partially.  So the odds that nothing goes wrong is actually smaller than the complement of the odds that every single individual machines works well.  Because you have to keep in mind lot of failures are cascading, if some of your nodes blow up then you know you increase the load on the things nodes and stuff happens.  If you have a power spike it fries a couple of PSUs it's probably going to fire more than one power supply, et cetera.
  So, when somebody says, that they pick CA out of the cap theorem, therefore, not partition tolerance, what they're actually saying is it doesn't actually work.  So, that thing is red, red means don't pick it for those of you with color vision issues I'm sorry the bottom one that you can't have.  Example of CP system would be zookeeper because it's CP you get consistent option, you get things like that you're doing locally except times they have to fail.  If you pick CP necessarily picking not A.  Not picking availability.
  The opposite is, for example, Cassandra you get inconsistent ops, sometimes stuff doesn't work as you would expect locally but they have much higher chance of succeeding.  Informally, let's take a look at five node cluster, five nodes, they're connected, cool.  So what happens when stuff fails?
  I still want to read and write, right?
  If anything fails then everything stops working that's not very useful.  So very common idea is majority core rum.  The idea is I have majority then we can still keep going, right?
  That kind of makes sense.  So, if you have a couple of failures, all the network connections leading from D and E have failed then you can still keep going, A, B, C they have got quorum.  I said earlier that network connections are pretty much indistinguishable from node failures.  You'll notice that if I drew it differently now the nodes have failed and network connections are just grey.  But this looks exactly the same for the point of the view of the quorum group which is A, B and C.  If there is too many failures then you can't get quorum, right?
  It falls over, it's cool.  You can still consistent reads and writes.  Except you can't.  So this is an example of failure that would break this -- sorry that actually happens reasonably often.  I'm not saying this specific thing but failure is where simple models like, oh, quorums, that's a consensus, right?
  In this case, A can participate in both groups there for D and E might be convinced, no we can still do writes we have quorum consensus, it's cool.
  This is not far fetched because real failures are typically partial and quite complex.  It makes sense for stuff to be partial, if you have 100 machines they're not all going to fail at the same time.  You're going to get one or a couple or handful that don't work.
  So, back to reality.  So, cap C is linearizability.  And caps A is anything operation on any node will succeed unless that node failed.  Those are extremely  strong guarantees.  When it gets really interesting is, you can't sacrifice partition tolerance, that's the bar on top.  No matter where you are, you alls get partition tolerance, you have to.  However, can have gradations between availability and consistency.  You can make trade offs like sometimes you don't need pure availability or consistency there's lot of really interesting stuff in the middle.
  You get to trade availability for consistency, another way of saying that performance versus ease of reasoning about the system.  Or like computer time versus developer time.
  Usually available systems are more scalable like lot of these databases like, for example, Cassandra if you need more write through put.  On the things hand, give up transactionality.  If your business logic  and trans-- you can't actually do.
  That there's no one correct choice.  What your application needs.  Example of creative sacrifice, is in a thing that people use to put configuration files in.  It should be consistency all the way.  It was found as a bug, this is my favorite example of something where people were argue can if it's a bug or future.  Gives you the option ever doing inconsistent reads.  What that means you're not actually guaranteed if you do the same read multiple times you're going to get the same thing.  So, maybe you're going to get some stale data, good news it still works under partial failures when doing consistent expensive read.  And you don't always need straight consistency, right some sometimes total okay that you get value from five minutes ago.  Now, the way that this stuff works is formally called consistency models we'll talk about those.
  We talked about simple once, this is the way Python works.  If I have a process and register, I say register think like Python name like X is some object F. I right X then I do a read, always get X back, if I write J, do another read get Y back.  That's how it works.  It's easy.  How you expect it to work.  Even if you have multiple threads or routines, multiple process eases you always get the most recently written value.  That's how we expect stuff to work.  How else could it?
  Now, the problem is that we are a spoiled bunch because this is actually an extremely strong model, which in a distributed system is extremely difficult to realize.  First of all information travels at the speed of light.  So those need little straight arrows those are actually a little slanted because it takes awhile.  You write X then written X do a read, you read X again.  Takes awhile.  When stuff is slow that means it can overlap.  So, for example, here, process B issued a read.  Process B is in a datacenter far, far away.  Process A is in the same datacenter, and process A issues a write, it's written, and by the time that that read that process B issued comes back like it's already read that new value.  So it's entirely possible that you read something that wasn't written yet by the time you started reading.  A little weird but we can work with that.
  Let's say multiple nodes that store a register you have one process, it's writing something to a node and then it issued a read, but, it hits the things node for some reason.  It's randomized or maybe that node is unavailable for whatever reason.  It is entirely possible that you don't read the same value that you have just written again because it hasn't been replicated to all of the nodes.  You can totally read old data it's a little weirder, you can still work with that.
  Lot of writes can get reordered.  Let's say that you have two processes and two nodes and you're doing an append on one side and append on the things side.  And when they get replicated it's entirely possible if we just pretend this is Python you list.append if you just shove appends around then any time that you read from node A you're going to see XY any type time from node is YX.
  That's a little weirder.  All sorts of stuff can happen.  Examples that were easy to draw.  But once you have registers, more semantics, more nodes, more node failures then you can absolutely get into a bunch of trouble.
  So, we need a way to reason about the system, right?
  We need to figure out what can and cannot happen, it's very clear that it's baste I cany impossible to write software if the contract that you get with that software is, well, I'm going to accept a write, maybe somebody can never read it again at some point in the future, we don't know.  Possibly.  Like it would be cool if we actually know what kind of guarantees that we get out of our systems.
  So, we need that.  And the sets of all of the things that could possibly happen is called consistency model.  This describes what you can and cannot do, what cannot and can happen.  These have been theoretically studied quite a bit, I don't think it's legible that was not the point of the slide I'm not going to talk about every single model on here that would be a gigantic waste of time.  They are however ordered by -- I mention the earlier you can have availability or consistency.  The stuff in red with the thick lines starting on top is stuff that you have that kind of model you cannot be available.  You can basically formally prove under some models, et cetera, extensive hand waving here, that if that is your guarantee then you cannot be available.  There will be times where your system does not work it will tell you like, I don't know what the answer is.
  Then there's stuff in blue, very thin lines apt the bottom.  There's stuff in recommitted, read uncommitted, those of you spent too much time with SQL databases.  You can be totally available and then you can see some of those effects.  The stuff in the middle in yellow with the medium thick lines is stuff kind of in the middle where you can have it while being available, but only if you guarantee that a node will hit the same -- that process will hit the same node every time.  So, earlier I had example of, okay, something weird happens here when you hit a different node and in between two processes if you can guarantee that that never happens you always read from the same node you can have slightly cooler consistency models in the middle.
  So, these are all fairly theoretical but they generally map to real systems fairly readily.  There is couple of words like, for example, serializability, that is important, I'm going to explain what it is.  It means that out of all of these concurrent operations that are going on so like a lot of stuff, lot of operations, writes, reads, whatever, serializability is the event results could have been produced by some sequential execution of all of those concurrent things.  If you imagine that instead of all of these concurrent things happening there is some universe where all of them happened in some specific order.  Any order.  And you would have gotten the same results.  It sounds really weak because I haven't said which order, there is some order out there that I could have done this in I would have gotten the same result, right?
  But lot of ordering of all of the events in the world.  That can be very weak.  If you start with the register at zero, I issue three operations that all succeed.  I write zero, I write one, I write two.  Then at the end of this it could be zero, it could be one, it could be two.  All of those are entirely correct.  I could say, look, I got the write that wrote two I got that last that is the value that I remembered.  Or I could have seen one last.  Or I could have seen zero last they're all compatible.  That is a useful thing.  However, the reason that it still works those are in the kind of separations that you particularly could with or serializable register.  A thing compare and swap.  Those of you have ever taken -- those of you who have ever taken maybe some assembly classes, like the CPUs have atomic swap operations.  What you -- operations that you issue are not set it to two, it's set it to two by the way I'm assuming that it's one right now.  And don't set it to two if it's not one.  Then wait, don't succeed, only succeed if it's currently one, then write two.  When you do that, then this set of very, very similar operations, there's only one possible way that it could be executed.  Because the second one is only one that can be executed because X is zero to begin with.  Then write one to X then you can do the third one because X is one now.  Then you store whatever in X put it in Y then finally do the first one.  There's only one -- despite the fact that is still only serializable where this is can be executed in the serializable system.  All the way at the end X is always one, Y is always two.
  Another nice one is linearizeibility.  I'm mentioning that because I mentioned it earlier as what the C in the cap theorem means.
  This is cap theorem, consistency.  And what linearlizeability appear to occur instantly.  Earlier when I said like, information traffic at C, the lines are skewed, yadda, yadda, yes, that's true, they will street still be skewed, but they will still take some time to arrive, et cetera.  But the system through various things guarantees which involves a lot of consensus, a lot of nodes talking to each things, et cetera.
  Appears that you are writing to a single register.  You never see this affect where, you see a still read or something like that.  Finally strong serializeability.  Is both linearlizeability.  I mentioned very briefly that your computer is distributed.  Even your SmartPhones like most of you have SmartPhones with dual or quad cores or whatever, those things are distributed systems.  You can see the affects.  You can even have these models in centralized systems that just have to deal with a lot of concurrent see.  For example, SQL databases you can config what kind of consistently levels you want, you can see things like read committed, read uncommitted, prevent certain things from happening or allow certain things from happening, this is all about trade offs.  You are trading performance, so when you say your SQL databases I don't care if I'm seeing data in the middle of a transaction or whatever, like it's okay if it's old or if it's new hasn't been committed yet, et cetera that means that your database can make optimization, is that it otherwise would not be able to make.  Convertly asking for very strong consistency model that necessarily implies that your database has to make all sorts of guarantees to make sure that you don't see inconsistent data.  That is expensive.  That requires distributed system that requires talking to all sorts of nodes making sure what they have, if you accidentally to go that node that you don't see a wrong value.
  So, Python doesn't really have a lot of strong concurrency tools there.  But another programming language that I like called closure is very -- banked on concurrency they have multiple kinds of references, so like when you -- Python you can say X is some object.  In Python only have one kind of reference, couple if you include like -- and stuff, in closure you can have multiple kind of references that only differ in what kind of consistency model, is that they allow.  For example, in closure you can have software transactional memory.
  You can run transactions like in SQL database, instead of the database you have to write SQL and limited by what your database allows within that transaction, for closure you can write arbitrary code and execute that over a reference.
  So, when -- this is fairly advance talk if this doesn't make sense your eyes glaze over, don't be afraid.  While doing this, I tried to figure out what the difference was between -- what the concurrency model were in Python, now, I'm a twisted core contributor, love twisted, and very common alternative to twisted is multi-threaded programming with a bunch of blocking IO.  I tried to figure out what the consistency model within twisted was.  And I'm reasonably sure that it's strongly serializable it's invent loop, has a single thread, this goes exactly -- also for like async IO and bunch of things ones I just looked at it for twisted.  There's an event loop with one reactor thread.  I note serializable some sequences which I could execute all of the stuff sequentially I would have gotten the same results.  Well, twisted does execute code sequentially it just does IO in parallel, but that's okay.  It runs code sequentially, that's what the reactor does it finds the serialization that works.  It's linearizable from the point of view in call back it looks like I'm the only thing that is going on.  Also when I'm in call back I don't see any things call backs doing stuff at the same time.  Now I wondered okay what is Python's consistency model.  If I write Python, what do I get?
  The answer is literally nobody knows.  Some people tried to figure this out for unladen swallow tried to get rid of the gill, they spent a lot of time trying to figure this out.  And basically there's a pep written, I don't think it's fully an official pep might be a draft.  But they talked about number of consistency models and the problem was that Python never defined what was its.  CPython had one by virtue of being a program that you can run.  It was very hard to figure out what are the guarantees that I happen to have because I'm running on CPython, what are the guarantees that Python gives me by default, right?
  So, it turns out that it is basically whatever your CPU guarantees, which is not really a lot these days except it's probably fine because there's a gill in all your objects live on the heap anyway.  So, maybe?
  I don't know.  Plus like all multi-threaded code has totally correct use of mutual exclusion systems, right?
  Fine.  Cool.  Let's talk about time.  We want time in distributed systems, right?
  Time passes, progress is made.  So, one idea of time is global clock model where you have a bunch of nodes, they're all interconnected.  You might not be able to see the lines, have to trust me.  And they're all talking to the same clock.  So the thing in blue is the thing with the clock.  Now, that global clock model means that everybody has a same clock and all accesses to that clock they're instant, uncertainty is zero, everybody sees exactly the same clock as really cool affect it means that if I see a time stamp from somewhere else, I can compare them.  This is like, you have one if something happened on Saturday another says it happens on the day after or -- says Sunday I can compare that, okay, cool, it was exactly 23.15.  You can think of it like an office.  There's a wall clock on the wall.  Everybody sees the wall clock.
  Everybody knows exactly what time it is.  Or you know if James bond movie everybody synchronizes their clocks before it gets started.
  Another model is the local clock model where no global clock but all of the individual nodes have their own clock.  And each one of those clocks is reasonably reliable, problem with this is that they will start to diverge, they're not exactly the same clock.  And so you can't really compare things time stamps you can get approximate idea but there's going to be some issues there.
  The mental model for this is top watch, you have a team, everybody gets a stopwatch, you know when ten seconds have passed but that doesn't mean that you can know when -- that something else happened ten seconds ago on this things node over there.  Then finally which is mostly theoretical model there's the no clock model.  Literally nobody knows what time it is.  We don't know how much time is passed we don't know what time it is, we know nothing.  Turns out theory ruins the day again, science ruining everything since 17 something that you can't have a global clock.  You can pretend that they almost exist but you're going to be wrong pretty often.  To illustrate that, Google has thing called spanner, it's distributed database, and so they use GPS and atomic clocks if you didn't know how GPS works, hint, it's really precise clocks.  What they do they have lot of GPS receivers like a lot of GPS receivers, and they have atopic clocks like literally fountain clocks somebody moved those into a datacenter.  And in their paper they say, atomic clock drift significantly.  When you think atomic clocks have significant drift you probably have a problem.  And then finally results are like, uncertainty is generally below ten milliseconds.  Okay.  So that sounds really good.  Ten milliseconds is not a very long time until you realize that there's 100 of those every second.  Now a hundred seconds, not all requests are banging upon the same thing, this is not perfect.  It's really, really good.  I'm not trying to hate on Google spanner they needed to do a thing, perfectly valid reason, my point is you can't get global clocks even if you have millions and millions of dollars to spend and somebody who is going to okay an expense report for like $10 million worth of atomic clocks.
  So, on the things hand you also can't have no clocks.  There is -- basically need them for failure detection, the way that works is you wait awhile and if you don't hear anything for let's say, a second or ten seconds or something, then you assume like, okay, look, either the network is hosed or the host is down, I don't know what's going on.  But I'm definitely not going to hear an answer back by waiting awhile you can differentiate between, is this just a network being a little slow or host being a little slow versus, is it actually broken and it's not going to fix itself.
  Obviously that's a -- it doesn't work all of the time you can't make it work all of the time.  There's an interesting theoretical result called the FLP result that says, I'm not going into too much detail, essentially you can't have a distributed system then because like you can't get machines to agree on any value ever.  And that's very problematic.
  Time stamps however are often a proxy.  You don't actually really care what the time stamp is, but maybe you do to display to human uncertainty is acceptable.  You don't need to know the exact micro second at which a transaction got committed to a database.  Probably if it was 5:00 p.m. on a Friday.  Time stamps very often don't really have to match real world time.  If you have a timeline, let's say this is a number of things happening to a single register, I'm writing X, writing Y, et cetera.  Now I've marked these as T0, T1, T2.  Those could be realtime stamp or just integers.  Very often doesn't care that this was the thing that was written on Saturday at that time.  My application compares this was version two and it's read version two and writing version three.  Cares about version numbers.
  You can get away with that.  Not the first person to have that idea.  These are typically called Lamport and vector clocks.  Informally again I don't really have lot of time to go into extensive detail on everything.  It means that you keep a version number of what you've seen.  You have the data, then you know like this is the 7th version of this data.  And a little bit more complex vector clocks means that you keep version numbers of everything that you have seen that things nodes have seen.  So basically whenever you communicate something, you pass along that entire big vector of version, is that you know things nodes have seen.  The robe you want this if you have lot of writes going together at least you can detect when there's conflicts.  Lot of systems like, for example, a distributed system, really good, should you use it, use vector clocks to figure out when stuff has gone wrong.  I do have some good news today, most of that was kind of bad.  Like a lot was theory, like, the cap theorem, you can't have nice things.  FLP theorem you definitely can't have nice things.  It's pretty bad scene.  But that said, we're here at PyCon and we have a lot of employers that run really, really big systems.  And so clearly it's not as bad as I had just said because obviously some people do actually manage to produce working distributed systems.  I mentioned Google, they have a massive like uncountable number of nodes.  And they're all talking to each things they seem to be producing the systems like chubby and spanner and they put out white papers and when I go to Google it usually works.  Obviously some of this stuff is actually right.  There is stuff that you can rely on.  And one very simple thing that a lot of systems end up implementing is Queus.  I'm not going to talk about queus there's a good queu talk they're all recorded go to youtube, watch one.  Basically idea is that you have too much work to do on some front end node so you just shove it into queu, somebody else's problem, somebody will take care of it.  Very simple paradigm it has very reliable software and it solves a heck of a lot of problems.  For slightly more complex things, you need consensus protocols, for example.
  Now, I talked earlier, you might remember in the example where I talked about quorum as a way of maintaining consensus in a cluster and I showed with the one where A could talk to both groups like, A doesn't actually work that way.  Consensus protocols are things that do actually work that way.  Consensus protocols are way to get a number of machines to agree on a value.  Turns out that it is about as hard as getting humans to agree on a value.  Real tricky.  There's couple of big ones, one algorithm is Zab, commonly used by zoo keeper.  Zookeeper is the thing that you should use.  It's on the JVM it has some operational issues in the sense that now you have to learn how to operate the JVH the alternative is significant worse.  Just use zookeeper.
  There is Paxos, for example, Google's chubby, distributed lock services use that.  I put that there with a star.  Paxos is notoriously to implement.
  Actually my job interview ended up basically me being ranting about four hours about Paxos and like the problem is, one of my interviewers who also knew Paxos we couldn't even really agree how many phases there are, there's the two phase camp and four phase camp like, who knows.  It's really tricky.   Other thing when you look at the papers, very often they don't implement Paxos they implement it with an asterisk with some stuff like actually that not even in the Paxos paper it turns out that otherwise you can't make it work.  Finally there's new contender, raft, I gave ETCD.  That is -- raft has really interesting because it was specifically designed as a design constraint let's make this easy to understand.  They have science to back that up.  They taught to a bunch of students, the students understood raft and did not understand Paxos.  That was actually really good.  Wonderful paper if you are really interested in the stuff you should go read it.
  Now, getting stuff to agree on a value, getting computers to agree on a value is not terribly useful all by itself.  You reasonable want something more than that, right?
  You don't want your computers to just agree on something you want something a little bit more useful and closer to your application domain.  So, zookeeper calls this recipes.  I've copied the term.  You can do this equally well in zookeeper as you can anywhere else.  So these are things that operate on top of consensus protocols, you can get things like locks, in distribute system you can have a lock.  Make sure that I am the only person doing this thing right now.  Can be very useful.  Sometimes application has to do something that is a side effect, right?
  Maybe you're like, I don't know, making some phone calls or sending some letters or even sending e-mail like you want to make sure you don't have erase condition you have two of the same requests that you accidentally send two phone calls or two e-mails.  Make sure that there is only one thing at a time doing that.  You can do that fairly easily with consensus protocols.  Another thing is barriers, the inverse of a lock in the sense that lock making sure that you're the only thing doing something with the barrier, you're waiting until everything reaches a certain point and then you can continue.  Owe it's like if you want to make sure that you don't end up incurring backlog then you can use barriers.  Another really cool one, I'm going into more because we use it at work, is set partitioning.  Means I have a set, I would like them to be partitioned across many of my nodes.  So, for example, you can have the numbers 1-5 and you just plug it into some software and each of the nodes will think that they own this particular -- one particular value in this case.  And when that fails, this is where it gets interesting.  When that fails the consensus algorithm recognizes that it failings, and redistributes.  So you can -- suddenly this one node has two elements to handle.
  So that's really interesting if you want to -- for example, we use it for, we get issued a lot of requests over our HDAPI we do some somewhere.  We create servers.  Obviously don't want to create too many servers because then users get overbilled you don't want to create too few their websites go down, either way users will be unhappy.  So, what we do, we use set partitioning to figure out given a particular request I.D., some idea that you get from the request, what does this belong to.  There is always going to be -- in that sense similar to a lock, always going to be one node that is an exactly one node that is responsible for distributing -- doing that particular piece of work.  That way we can guarantee that you don't accidentally, for example, if a customer says, hey, can you get rid of two of my servers that we don't have two nodes, sure, I can get rid of two of them and end up killing four.  Something I'm real excited about, CRDT.  They are conflict free replicated data types.  The problem is, so far every time I've been talking about operations, you read something, you perform some computation, increment a number or whatever then write it back.  When you have multiple concurrent operations that means that once in awhile you'll get a conflict, right?
  If I have two operations that read at the same time, they do something they write back suddenly I have these two values they're not necessarily the same.  What do I do.  One solution to slow one of them away doesn't really work too well, typically call last write wins, or most writes lose.  And the things alternative is coordination, right?
  You can make sure that everybody basically go around the cluster says, yes, I own this value now, do I own this value now, I do two phase commits, lot of back and forth like TCP but worse.  That's really expensive, right?
  What we talked about earlier, the trade off between availability and consistency.  When you pick consistency you're going to get rid of availability and performance.  So, I want highly available data stores but not nonsense data in my database.  Seems like a reasonable thing to want.  Turns out that is something that researchers only really fixed like the last five years or so.  So the idea of one of the ideas is that you describe what you want and you  describe how to resolve event conflicts.  Instead of what I do before which is, I read a thing then do something to the thing then I write the result back, you're going to say, please go ahead and do this to the thing.  And then you can do a lot more interesting things.
  So, CRDTs are specialized there are two ones as far as I know only two one, is that have been defined they can be commutative RDTs going to broadcast the operation operations, the merge operation that can merge multiple of those into one data point.  This is going to be commutative I can flip the order, can go to associative, doesn't matter what order but not necessarily item -- if I apply operation multiple times I'll going to get a different result back, or might get a different result back.  Simple example is Integers, if I have the list plus one, minus two, plus five, minus four, no matter what order that I add those together I'm always going to end up with three.  However, if I see any of the operations more than once or not at all, then I'm going to get the wrong answer.
  That is an example of a simple CRDT.  Alternatively convergent RDTs will broadcast states, some can be partial, like can I don't know what the final results nobody knows.  But I'm going to tell you what I have right now and then when you get that value that I just sent you you're going to merge it with whatever it is that you know.  And eventually if we just put all of our minds together we have all of the information that we need.  So, this operation is commutative, associative, you can apply it lots of times.  Eventually you'll also get to the same results.  Doesn't matter what order you do stuff in.  For example, if I have a set on top with the number one in it.  One writer that adds two, another writer that adds the number three.  And there's only one way that I can merge those two sets eventually I end up with one, two, three.  If I hadn't had CRDT just doing last write ones, one of these two writes would have been lost.  You can do more complex stuff.  So, this is -- here I'm writing on left side I'm writing numbers one, two, three, four, separately.  They get merged neatly like that.  On the right side there's a node that has network issues or something, it's all one, two, from some node, one, two, three from somewhere else.  And you'll notice if you just merge them, just keep merging, keep applying the merge operation if you eventually see all of the writes then you will always end up with the same set.  Now in practice it's virtually always a Convergent RDT, the kind that commits states.  I'm not sure why that S. I have a theory.  The theory is that -- hard problem with the Convergent bit is writing the actual algorithm that does that.  You have to write the merge function, figure out what your internal data structure is.  That means you have to solve local problem once.  But you have to solve that local problem.  With the commutative thing, you have to make sure that all operations are delivered exactly once.  That means you have to solve really hard distributed systems problem all of the time.  So, as basically the crux of this talk, distributed systems, trickier than you might think, in practice it ends up being Convergent RDTs you can actually use them you don't have to solve that hard distributed systems problem of making sure that each operation arrives once.  There is a lot of examples of these.  I'm not going through all of them.  The counters, count up and down these will be consistent.
  You can have sets, there's couple of different examples of sets.  You can have maps, really those are just implemented sets, that's kind of cheating.
  You can have graphs also just implementing sets.  Kind of cheating.  You can have registers the super simple example of what I was talking about.  Notice that last write wins is technically a CRDT but like not very useful one obviously because occasionally you will throw some data away.  Finally there's sequences, really cool.  Sequences stuff like a list and these were developed mostly for operational transforms which is the cool bit of technology that makes it so that when you write stuff in either pad or Google docs at the same time with multiple writers that that actually works so basically try to store stuff in the middle of the sequence.  That is probably one that's most complicated.  If you're interested in this there is paper on basically all of these, most are fairly simple to understand.  The sequences one is a little tricky it comes at the end of the paper for a good reason.  Finally, using CRDTs like designing them is tricky, because they can however is fairly easy.  I already plugged Riak I tweeted it's hard to do introduction of distributed systems talk without repeatedly plugging Riak because it's awesome.  It has flags, registers, counters, sets, maps, et cetera.  They are fairly easy to use.  So, to wrap up.  Yeah, distributed systems.  They're more resilient, more performant and more importantly make some problems distractible.  No amount of really fast computer is going to make Google something that you can do on a single machine.  Or Facebook or any things large distributed system.  So there's stuff that we can do that we can't do any things way.  But, distributed systems.  Can be very hard to reason, huge state space, the amount of possible orderings of all of the events in the world is pretty much infinite.  There are no repeat scenarios, very, very hard to debug.  You can also be expensive to operate, five machines at least five times as expensive as one machine.  But, lots of distributed systems.  Severing about trade offs.
  What is right for your application.  Don't build what is off the shelf, stuff like zookeeper you're not going to get it right.  Use the tools that are available.  Why distributed systems?
  Because you're out of options, pretty much.  Lot of us are.  Don't take that as advice, don't do distributed systems.  That's all I got.  Thank you.
  [ Applause ]
  44 minutes, 57 seconds.  In 45-minute time slot by the way.
  >> I'm afraid we have no time for questions.
  >> Unless you can fit in those three micro seconds.
  >> Thank you all.
  >> I will be taking questions like outside, just grab me.  We do need to vacate the area for the next speaker, though.  
