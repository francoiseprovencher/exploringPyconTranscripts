  >> Hello, everyone.  So our next speaker is Andrew T. Baker.  His talk is called "Demystifying docker" please give him a warm welcome.

  [Applause]
  >> Great.  Thank you.  Docker is really interesting technology and there's a lot to it.  Way more than we can discuss in 25 minutes.  But my goal today is to kind of give me enough to understand the basics and sort of demystify anything that might have been confusing you then give you appetite to go try it out yourself.  So, first a little bit about me.  I'm Andrew baker a software developer in Washington, D.C.  I'm currently independent.  I did consulting for most of my career then took time off last fall to work on my own projects.  One of those was an introduction to Docker video tutorial with O'Reilly.  Which is kind of like a two hour crash course to get you started with docker is available on their safari surface.  I don't work for Docker but I've been using it for a year and a half I think it's really interesting technology and turns out lots of people in this world want to know about Docker and he really like teaching people it's a good match for both of us.  Let's talk about you.  Who here has done the official Docker tutorial on Docker's website?
   maybe quarter of the room.  Who has installed Docker on their machine?
  Okay.  And who has written a Docker file for their own application?
  Okay, yeah, pretty good.  About a quarter all the way across the board.  We're in the sweet spot for this talk today we're going to talk about what is Docker exactly.  We're going to go through some sample workflows of how you deploy applications with and without Docker.  We're going to kind of talk about the story of Docker where it came from and where it's going.  Then I'm going to wrap up with five tips that you should know before you try out Docker for yourself.  Who here has said this phrase to one of their co-workers before?
  [ laughter ]
  I would imagine most hands in the room would go up if your your co-workers were not next to you and still bitter about that production defect.  This is one of the worst parts about being a software developer when something goes wrong because of an environment mismatch and even when it's two good engineers you're both human beings in the back of your head you're thinking who is is blame for this production level defect that management breathing down my neck now.  I think docker is an interesting technology but for me as software developer this is the most compelling reason to use Docker.  Because it helps me solve this particular problem of making all of my environments much more consistent.  So, the promise of Docker you might have seen before is "Build once, run anywhere" the central concept this idea of software container when you have this on your development machine you as developer can wire up everything inside that container exactly the way you want.
  You can install your operating system level dependencies like which version of Python you want.  You can install your Python dependencies, you can install application source code.  When you seal up that box wherever it goes, whether it's another developer's machine, server in the office, in the cloud, going to behave the exact same way.
  In my one and half years of experience in using Docker and deploying that this holds true.  What is Docker exactly?
  When most people hear about Docker they try to compare to existing fool that they already use.  Some people think it's like a virtualization tool like VMware, or virtual box.  Some people might think it's VM manager like vagrant.  Compare to Ansible and C groups, LXE or go.  The truth is that docker is kind of in category, there's not really anything out there that is quite like it.  If you go to Docker.com's website he will tell you open application for developers and sys admins and which helps, it's about developers and Sys admins but still doesn't quite paint the whole picture.  For me I like to think of Docker occupying a spot on a spectrum.
  This is a spectrum of techniques for deploying a software application to a server.  All the way on the left side are techniques that are less portable but have minimal overhead epitome is manual configuration.
  This is when you spin up a new EC2 instance in the cloud or you new server comes in the mail to your office you want to box it and boot it up.  You have to get access to that server, run all the commands in the right sequence to configure your application.  All of its dependencies, all the operating system dependencies when you're done red ready to go production.  This technique is great, it's straight forward, probably how all of us configured our first server.  But it suffers from a lack of reproducibility.  When you spin up that next server you have to run all the same commands and if you're human probably error prone and you are liable to make a mistakes.  Conventional solution for the past ten years has been configuration management tools like chef, puppet, these tools when you're using these tools you define a desired end state for your server using their specific configuration language then when you point those tools at a new server they go off configure everything for you.  These tools are great because it means once we've done the work to configure one server, but they all have the catch of you have to learn their specific technology, you have to know how to write a puppet file or Ansible playbook to make them work when you spin up a new serve he have to wait for that tool to run before you can get in there use it.  All the way on the right side of the spectrum are techniques that are most portable but laugh lots of overhead this is where traditional virtual machines lie.  If you work at company where your build process creates Amazon machine image, then you take that Amazon machine image which is full virtual machine image and deploy to EC2.  And virtual machines are great, because you know that wherever they run, they are going to behave the exact same way because they think that they are a full real computer.  They're completely isolated from their host operating testimony.  But operating system.  The virtual machines disk image size is the same if you would need installing a server on a hard drive in your office.  It's big.  And they take a long time to boot up and they take more time to boot up the more services you have running on them.  So, to me Docker occupies a sweet spot on the spectrum where you get most of the benefits of portability and isolation of traditional virtual machines at the fraction of the overhead.  Docker containers are much smaller footprint than traditional virtual machine and start up in the blink of an eye.  Benefits beyond solving problems for impatient people like me, -- impatient but how you can spin up applications. How does Docker pull this off?
  We don't have time today to go into the full details of how Docker manages to do this kind ever container virtualization.  But if you have been to Docker.com you've seen a diagram of the hardware at bottom, post above that hypervisor layer, which is using the computing power of your server to provide a virtual hardware that is then consumed by a guest operating system.  That guest operating system is where you as the developer install all of the dependencies for your application.  So, great for isolation, but redundant operating systems, we're running a host operating system and guest operating system.  With Docker the stack looks shorter we still have server hardware and host operating system instead of hypervisor we have the Docker engine, it's relying on some other open-source tools to do these -- create these software containers which are in sense smart enough to leverage the components of the host operating system to create what looks like a fully isolated virtual guest operating system for the applications that you run inside of them.  So, like I said we don't have time to go into the full details today but they're out there.
  I think one of the cool things about Docker is that it wasn't the first open-source project to deal with software contain bit was the first to make it accessible to people who aren't Linux expert.  Then I definitely would not be standing here before you today.  Now I want to walk through couple workflows how you might deploy a software application first using just traditional work floor maybe with configuration management tool then using Docker containers.  To set the scene, we have three different environments.  Our application is a Django web application, and uses served up on the front end with engine X for reverse proxy.  And we use a small server as a cache.  On developer's machine on the left they're probably developing using Django development server managed up high run server hopefully fooling two scoops advice running real post instead of SQLite.  Then all the way on production is where we have all the components in our stack that might be the first time in our workflow that we see all those components in play.  Let's walk through how code update would propagate through this workflow.  First, a developer writes a new code, looks good, commits it.  Pushes it up to our Git server and the master branch in Git repository points to the most recent commit that the developer just pushed up.  After that, our build server leaps into action.  Pulls down that latest code, runs our test suite and then if the build passes we put a green dot saying everything is cool or pull request, we say good to merge.  One thing that is already kind of tricky about this situation is that you might not -- unless very careful might not have the same version of postgres running this is especially tricky if you are using outsourced configuration --  like Travis or circle CI.  I noticed when postgres 9.4 came out wasn't immediate that Travis and circle had support for 9.4.  If you had a new developer on your project set up their machine for the first time would have ended up with 9.4 unless you went out of your way to tell them that we're still on 9.3.  So, after the build service says everything looks good we go ahead take that new code into production.  Might news configuration management cool for keeping things system would SSH into our production servers our database server or application server and deploy that new code restart all the services necessary.  In this diagram each circle is a process that's running in each environment.  With Docker the workflow looks a little bit different.  Notice right off the bat a new box in the top right corner called the dodger registry.  See how that fits in a second.  You'll also notice that on our development machine we're running all four components of our stack.  And we're not just running those little white circle processes right on our native machine, inside Docker containers.  So we have a box, Docker container for each process in our stack.  And that is sort of rule of thumb when you start working with docker is one process per container.  So, let's see how our workflow looks different now that we're using Docker.
  When we have new code our developer would still push it up to GitHub, that is the same.  Our Git repo update with latest commits on our master branch.  Our build server would pull down that latest code.  But then when it finishes the build it still going to probably put that green dot back to GitHub to let everybody know that everybody is okay.  But also going to produce something new.  Going to produce something called a Docker image.  It's going to push that Docker image up to the Docker registry that could be the Docker hub which is the registry that Docker incorporator maintains or your own internal registry that you stood up yourself.  This Docker image represents not just a snapshot of your code at a point in time like a Git commit does, but also a snapshot of the entire -- entire environment that that application was tested in.  And so that is a the artifact that you can be confident will behave the exact same way introduction as did it on your build server.  Now when we to go deploy to production we'll see that again all of our processes are running inside containers instead much reaching out to Git server to pull down our latest code we're reaching out to the Docker registry to pull down the latest full good image of our code and environment it ran in and we're going to spawn a new container in that production environment based on that Docker image.  This could be a little confusing like I said to get your feet wet just jumping into it and trying it out yourself.  One point I want to stress before we go any further is the difference between diners and images.  This is by far that is most difficult to learn when you work with Docker for the first time.  The rule of thumb is that containers how you run your application, it's the only way any work gets done in the Docker world.  Images are how you store your application, and you how you move your application, move those fully-baked safe isolated environments between different servers.  So, on this previous slide, all of these boxes are containers in our development machine we're running a container for each process on the build server we're running a container for each process, production we're running a container for each process.  But the Docker image is how we propagate new changes between all those environments.  So, when you start working with Docker yourself, the first thing you need to do before you can take an application and put it inside a Docker container create a Docker file for it.  A Docker file is a special file that lives alongside your source code which basically tells Docker how to create a Docker image for your software application.  And when you look at the commands here even though you've may have never seen a Docker file before, it's easy to interpret.  Some people described them as almost glorified bash scripts with special commands.  So, all the way at the top, each Docker image has to start from auto base image in this case our base is just a plain stripped down installation of the Ubuntu Linux.  After that we run three commands to install.  Then we tell Docker which port is the relevant port to expose from containers that are created from this image Docker what command to run by default when it creates new containers from this image.  Now, one of the really, really cool things, maybe my favorite thing about docker is that you don't actually have to start from Ubuntu from a raw Linux distribution when you create your own Docker images there is this really awesome thing called the Docker library, which is a collection of official images maintained by Docker and other open-source projects for almost every kind of programming planning your server or appliance.  There's one for Python, one for ruby, Java, HPH also ones for postgres , Mysql.  Apache and there's more and more being added all the time.  These library images provide to you a base image that already has Python stalled in this example you don't have to worry about installing Python you tell Docker I want to build my images from the Python:  3.4 image when you start configuring your own application with inside that container has Python 3.4 installed for you.  One of the really cool things about these base images is that they actually have extra special sauce, which are called the on-build images.  They come with some extra commands built in to help you configure your application.  So, when you use Python 3-on build you can go from zero to Docker on a software application in two lines of code because Docker will look for requirement.txt copy it into container, install your requirements then look for your source code in the directory that is along side your Docker file, copy that into the container you're ready to go.  All you have to do is specify the base image that on build and default command the dock sheer run when it creates new containers from that image.  Now I want to shift gears talk about the story of Docker.  Because I think it's really cool story it's especially relevant for this audience.  So, Docker has familiar origins, it was revealed at PyCon at this very conference two years ago in 2013.  Solomon hikes what was the founder of dotcloud did a lightning talk, I recommend going back check it out on Py video he talks about how dotcloud runs a platform as service, it was a company that was often mentioned in the same breath at Heroku and say how can I do the kind of crazy stuff to run a platform as a service.  For long time they said, it's really complicated, you have to use Linux containers and create this open-source project called Docker to make it accessible for people like us.  And after that, everything is history, Docker caught fire and became one of the hottest open scores projects from last year.  Amount the end of 2013 dotcloud changed to Docker and rebuilt its whole business around open-source project that is Docker and spun off a platform as a service business to another company.  So, since then, tons of open-source projects have been created using Docker.  And obvious application was using Docker to run a platform, because that's what it did to dotcloud.  Look at Flynn or Dais are good options to stand up a platform as service within your own company.  Another cool story is the story of Fig which is open-source project built upon top of Docker that made the easier to run multiple at once, stand up that whole development stack of application server, engine X, cache, database.
  Fig was so successful that it was actually acquired by Docker is now being rebranded as official Docker tool called Docker compose.  But there's lots more, new configuration, continuous he integration services, all sorts of crazy stuff.  One of my favorite application of Docker is eliminating some of the hassle of using dependency-heavy utilities.  On Wednesday morning I ran a three hour tutorial on Docker here at PyCon at the end of the first exercise students created a Gif using a Docker image.  I found this command line utility when I was looking to make a Gif for myself a few weeks ago and looked really good.  But it was a node tool, not only was it a node tool had three additional system level dependencies I would have had to install.  I would have had to install FFM meg, convert, which was a binary that some part of image magic.  I would have had to install some fork of another repository.  But I was so pleased to see right under the list of requirements or you can just use this Docker image that someone out there created for you.  So took the process of making a .gif using this tool from installing node, installing all these packages to running one command with Docker pointing that container to my movie file and then outputting the Gif I'm ready to go.  So, starting to look forward a little bit in 2014 Docker's priorities were get to 1.0 production ready.  Take that big red notice off the documentation which says "Use at your own risk" that happened last summer.  Establish partnerships throughout the industry, so it's easy to use.  Round out some rough edges that made it harder for newcomers to use Docker.
  In 2015 it seems like the priorities are making easier to use all around.  Continuing to round out some of those rough edges that make it hard for people who are new to Docker to get started with it.  Popularize best practices for scaling this is what I'm interested in as someone what has been using Docker some time.  When it hit 1.0 last June, it has been about nine months, almost a year.  Since that happened, and so big companies are starting to use Docker in production for serious things.  And we are all very excited to hear them come back with some stories how they have done it and also going to see lots of new open-source projects pop up of tools that people use to help them do this.  So, it's going to be really exciting to see how Docker breaks out this year and has really great stories abut how people are using it at scale.  Docker this year is also trying to include more batteries.  So, Docker is adding three new tools to its official stack.  Called Docker compose, which I mentioned earlier that previously known as fig.  Docker machine, and Docker swarm.  And these three tools are going to help you as a developer go from zero to Dockerized application to your own Docker cluster across multiple hosts without having to reach out to any third party tools.  So, Docker is definitely not trying to cannibalize to help you run your Docker diners but trying to give you the developer all the tools you need to get started on your own.  Which I thought this is a really great idea. Now I want to wrap up with five tips on how you can try out Docker for yourself.  
  First, when you're trying to pick a component of your business to try out using Docker, I would try to go all in on a non-essential service.
  Don't just use Docker for your developer environments.  If you have complicated environment with lots of moving parts then Docker could still be useful to you.  But one of the catches, one of the strange things about docker is that all of the work you do to Dockerize to get your stack working locally is 90% of the same work you need to do to take those containers out to the cloud.  So it can be frustrating when you're now Docker if you're spending all of this work making Docker work on your development machine but you don't get to see this pay off of seeing how easily you can use your old stack consistently to other environments.  Two, make sure you use those official base images like I talked about.  Like you saw in that Docker file for MongoDB it's easy to build don't be afraid to discard those official images when they can't do exactly what you're looking to do.  And the cool thing about the official images that they're all open source you can see all of the Docker files that were used to create those official base images which I often do when I'm trying to reference Docker file best practices.  Three, install Docker compose right after you install Docker.  Docker compose formerly known at fig is really, really essential for running a realistic development environment on your development machine.  It is the only way pretty much that I run more than one Docker container linked to each other on my development machine.  I will use the regular Docker run command, regular Docker command line client to run one off containers when I need them for the most part when I'm working as developer and using my Docker stack, I am using Docker compose.  Don't overlook this one.  Four, keep an eye on Docker machine and Docker swarm, other two new batteries that is included this year.  And number five, if you're looking to dig in real soon check out all the materials from my tutorial that we did Wednesday morning.
  One of the cool and exciting things it moves really quick as an open-source project.  But one of the drawbacks that have is that a lot of the tutorials that might be high on Google results are a little bit out of date don't have the best practices. I don't think this one is going to have a shelf life.  But hopefully you can at least use it for the next month.  And that's all I have.
  [Applause]
  >> We have a few minutes for questions if you have got one please come to the microphone.  Right there.
  >> Audience: Thanks for the talk.  Quick question, going back to your build workflow.  Basically you've got it set up such that every time you do a release build you're also rebuilding your Docker image from scratch.  If you go look at your Docker file that involves taking a base version of Ubuntu running which pulls in bug fixes and changes from the underlying Ubuntu, OS distribution.  One can easily imagine situations where you're deploying new version of your app you don't want to pick up changes to MongoDB until you have chance to vet them.  And lot of times you want to get bug fixes for underlying OS issues without having to redeploy.  What are best practices for managing the changes in those dependencies independently?
  >> Sure thing.  So, I didn't go into it in great detail here but one of the great things about all those base images all the official images that Docker maintains they all have liberal use of tags on those repositories.  They normally correspond to exact bug fix point versions for each of those appliances and programming languages.
  That's the best place you should start to make sure that you know exactly what you're getting every time you're building your Docker images.
  I've been pretty satisfied when their new Docker -- sorry, like bug fixes, new releases, new bug fixes releases and minor releases of those programming languages and appliances that those changes show up pretty quickly in those official repositories.  For me that's been sufficient.
  >> Audience: All right, thank you.
  >> Audience: So looking for some guidance as far as best practices.  I have an app that's using engine X to talk to Django via micro whiskey with over Unix socket, putting engine X in one taken or and Django in another container and as far as sharing that UNIX socket should I use a volume that share between the containers or should I scrap that entirely and use TCP to go between engine X and micro whiskey and live with any performance differences.
  >> That's a great question.  It seems like the practice that most people are using with Docker is doing links between containers using TCP.  But I'm sure I've read something on the Internet which shows you how to do it if you want to stick with the socket.  I think you have some options.  
  >> cool.  Thank you.
  >> Yeah.  
  >> could you talk more about the distinction between an image and a container, you were saying you run one process per container but am I understanding correctly that an image would actually specify multiple processes and multiple containers described is that right?
  >> That's not quite right.  With each image you're building -- if you're adhering to that best practice each image you build will have one process that you intend to run in it.
  >> Audience: So the dependencies of application would be multiple images?
  >> Yes.  When I am working on -- when I'm working on a Django app that I'm using Docker for -- I generally will use the official base images for postgres not trying to do any customization, something like engine X where I want to plug in my own file I'll write my own Docker file to drop that configuration file in that is own separate repository.  You do want to create a Docker file and image for each element of your stack that you want to customize using Docker.
  >> Audience: Okay, thank you.
  >> Hope that helps.
  >> Audience: My name is Luke.  I was thinking earlier you mentioned that you want to have one container per process.  It's a common Docker philosophy, what do you recommend about running services with containers, do you want to run them inside the container or outside the container?
  Like a basic service like keep alive?
  >> That's been one of the most interesting things to watch in the community as Docker has evolved.  When Docker first came out there was a very popular base image created by the people who do the fusion passenger project, which sort of made it easier to run multiple processes within a single container.  And that base image is still out there it seems like people are still using it, but it seems like the conventional wisdom is you don't need process managers or services inside your containers, you should build it to run in a non-demonized mode whatever process you're trying to run inside that one container.
  >> Audience: Okay, thanks.
  >> Audience: Hi.  This is going to be a fairly newbie-ish question.  Does Docker have any support for development and/or deployment on Windows?
  >> Yes.  Definitely not a bad question.  It was really funny last year seeing every week seemed like new big player in tech that was announcing they wanted to partner with Docker.  Microsoft definitely didn't want to be left behind.  So from what I can tell Microsoft is working on figuring out how to create their own sort of container technology that would be available in Docker.  But it would be -- not be a situation where you could run a Windows container on a Linux machine or a Linux container on a Windows machine.  It would essentially be two separate container implementation, use the same API that Docker provides to do it in both environments.
  >> Audience: Thank you.
  >> It's something to watch.  That's all the time we have.
  [ Applause ]
  Thank you.  I'll be hanging out up here if you have if I more questions, thanks for coming by.
