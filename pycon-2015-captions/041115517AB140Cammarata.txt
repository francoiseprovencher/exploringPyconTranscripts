  >> All right, hi everyone.  Welcome back.  Achieving continuous delivery:  An automation story.  Let's give a warm welcome to James.
  [ Applause ]
  >> Hi, everybody.
  Thank you for attending my talk.  Really appreciate it.  So, a little bit about me.  I am James Cammarata I'm currently director of core engineering with Ansible, Incorporated.  Took over that in January, late in February.  I've been a Python developer since 2003.  I was hired to convert some Perl scripts to Zope set up actually of all things.  So, about 2008 I started getting into open source.  I had been for awhile.  But found out about this cobbler project, my day job at the time was kick starting systems a lot, friend of mine told me, said I needed to look into it.  Did.  Ended up sending patches back.  Got to know Michael Dehan who started that project.  In 2010 that project was a little bit of drift I took it over, became the project leader.  Michael knowing me in those days when he started up Ansible, started up was looking for Python developers, knew me, approached me, I joined Ansible in July of 2013.  Been doing it ever since.  So, what is continuous delivery?
  I'm going to keep this a little bit high level since this is a novice level talk I will go through define all these terms.  Continuous delivery is basically the goal is to reduce the impact of deployments by doing it faster and more frequently.  How many people have worked at a large corporation where you do quarterly releases and everybody gets together for the big weekend deployment.  QA signs off, user acceptance signs on everything get into the war room, something breaks.  So, you end up in worse case scenario having to roll it back do it all next week end all over again the higher ups are very upset and want reasons why it failed and worse case scenario it takes you as long or longer to roll back than it does to do the actual deployment.  I know that previous company, the basically deployment took a day rollback took entire day to restore from everything back up.  About a terabyte of data it can be very painful when those big bang deployments fail.  It was popularized by the book "Continuous delivery" full title is there.  In 201.  The concept had been around a bit but their book really brought it into the mainstream, excellent read.  I highly recommend it.
  So I'm going to show you a little video here of kind of how I envision continuous delivery working in a very real sense.  So, I don't know if anybody has seen one of these, they call them the great ball contraptions for Lego.  It's basically very intricate ways to get a ball from A to B and people send lots of time creating these set ups.  I think this is the perfect metaphor for a lot of production set ups where you have a lot of things interacting in very difficult ways.  And this video I've probably watched it ten times this week because I just love it.  When I'm doing QA I'll let the whole thing play.  Basically shows this red and blue ball going all the way through this contraption and all the very myriad ways it does it it's really cool.  I'll show more of that a bit later.  Yeah, at the very end the balls come back in and dumps back in the bin where they started and whole process starts over again.  That's continuous delivery in a nutshell.  Obviously I'm with Ansible.  I'm very big proponent for many reasons Y. use Ansible in your continuous delivery set ups.  The first reason, reason Ansible to my opinion the most popular -- one of the most popular configuration systems out there is it's simple to learn.  You don't have to install anything on all of your end point systems.  Ansible uses the SSHD to go out and contact all these systems and write a little bit of Python code on them.  So you just need SSH and Python.  Some cases you don't even need that if you can execute using the raw module but I digress.  Second, the playbook syntax, the syntax overall in using Ansible is incredibly simple and easy to learn.
  It's all Yammel playbooks, very linear tasks, one task executes after another.  You don't have to worry about odd ordering, Ansible tries to determine, you're the order you list your tasks is the order they're executed in.  So, it can be very, very simple to pick up that way.  Of course, the downside to simplicity can mean that it's useless.  And in Ansible's case it is far from that.  With Ansible you can orchestrate some very, very complex tasks, pretty much the gold standard one we always talk about is say you have 100 web servers and you need to do an upgrade and rolling reboot of them.  They're behind a load balancer, so you say, okay, five at a time I'm going to take five servers out of the load balancer, upgrade them, maybe reboot them, do other things.  And then when that's done reinsert them into the load balancer then move on to the next five.  Doing things like that is what Ansible really excels at beyond just simple file packet service manager which every management system can do.
  All of them do those very well.  Similar situation that came up somebody brought it up that they used it for was migrating databases.  For instance, you can -- if you want the migrate your database you can use Ansible insert a new primary, take old one out you've got your new master database.  To upgrade to larger instance.
  To use with Jenkins or not rebuild their golden images whether they are AMIs for EC2 or heavy weight VMs or docker containers, everybody is talking about docker.  Very down to see that in use as well I'll talk about one of the case studies I'll go into later actually does that.  Finally obviously in that same scenario where you're operating on your systems in addition to taking out a load balancer you obviously have to deal with your monitoring systems so you don't start alarming when things stop, start, reboot.  Maybe detects system is a little degraded.  Ansible can just as easily be used to add and remove systems to your monitoring services as you're going along.  So next is reusability and consistency.  When you're writing Ansible playbooks they're generally very general.  You don't have to tweak them a lot if you run them on this set of servers versus on this set of servers.  Your developers and whatever environment they're standing up to do their testing, their development work can very easily use the exact same playbooks that you are using to set up and configure your systems in your production environment, staging environment, QA, wherever you're working.
  Likewise your CI system whether it's Jenkins or Travis can use those playbooks as well, that way you're guaranteed degree of consistency across all of your environments and you're just removing variables that might impact your deployments at a later date.  Finally extensibility -- not finally, but extensibility.  Ansible comes with around 250, I haven't counted recently but there are a ton of modules that we include with Ansible.  Anywhere from doing like I said the simple file package service management on a wide arrange of distributions, even Windows now.  All the way up to deploying full cloud environments in Amazon or Rackspace, several others.  We support pretty much every one of the major ones.  Inevitably you might have in house service, smack you've written in rest or something else that you want to interface with and you want to write a module for it.
  Women, Ansible is -- well, Ansible is extremely easy to write modules for.  You can take a look at the modules we have, they're typically very simple, straight forward little Python scripts that if you have been writing Python for a year or two you have no trouble understanding what they do.  Not a lot of complexity to those, usually.  Complexity usually comes in with the API you're dealing with, obviously, EC2 using Boto can be a little complex.  That's because there's a lot going on there.  Finally, others have of already done this.  I'm going to go into like I said case studies later on, couple companies that have publicly advertised that they do this.  There are a lot of others that I'm not even going to cover that have blogs out there where they go through how they're doing all of this with Ansible, or other systems and they have, in the case of Ansible they have a lot of times shared their configuration set up on GitHub or other places, you can go grab their playbooks and tweak them as you need and basically get up and running in an environment very, very quickly.  Also, Ansible is user community, has just exploded over the last two years.  I think we are very frequently hanging around well over a thousand users, in IRC channel our mailing list at about 2,000 subscribers to it, a very, very proud of our community, it's -- all open source projects have worked on it is by far one of the best, it's always very cordial, there is -- everybody is always very nice.  Beginner questions aren't mocked at all.  So it's very easy to get help from our community I think.  So, some of the common patterns you'll find with continuous integration nowadays, this is from one of our blogs on it kind of shows a high level view, I won't get into it too much.  But the continuous integration system, obviously is the cornerstone to continuous delivery.  Lot of people use Jenkins or Travis nowadays.  Atlassion has bamboo and several others.  But that is basically the system that does all of the work for you, it's your automation system and whether you have it doing just basic tests, building things or very complex multi-step jobs over matrices of distributions and other things like that, most of those will get you what you need.  Obviously we use Jenkins internally and do things, we use Tox and some other things to do testing across mull time Python versions whenever we do Ansible deployment  we do it all through Jenkins to upload our  Pypi and all that.  It's a very, very useful system.  Obviously to room full of developers don't have to discuss the importance of source control.  One of the important things to remember not only keep your application and source control but to also keep your configuration management information in source control.  As we call it now, infrastructure is data not code bus we look at playbooks at one level higher than the infrastructure's code because they're so simple to write and just really higher level abstraction to us than say writing chef configuration which is ruby code.  Second important consideration in source control is to integrate it with your -- depending which one you're using we always use Git, but to integrate it with web hooks or automated commit hooks, whenever admit to  pushed into your source control repository it kicks off a chain reaction of events that does your automated testing, maybe deploys to a couple environments. Maybe even as far as like it says at the bottom to get pushed all the way out to your production systems if no errors have occurred along the way.  Some people don't like that.  Some people like having that human gap between production.  It depends on what you're comfortable with if you're really confident in your automated testing then you can pretty much go the full mile.  Which brings me to the next point automated testing.  Obviously this is how you have that continuous integration system working and confident that the code you're pushing out does what you expect it to.  So, integration testing, unit testing and user acceptance testing doing things like ghost and Casper JS if you're doing web interfaces if you're doing angular JS, running your angular JS tests, what have you, it's really important to make sure that you have confidence in your automated testing. suite.  So, monitoring is probably one of the most overlooked aspects of continue deployment.  One of the most important because when you're pushing out releases very, very quickly you need to know when something went wrong.  So, this graph is from Etc Codus blog how they capture this information not only from the systems the logging but also from their deployment systems which as you can see in that graph of the vertical lines each one of the vertical lines is a deployment.  You can see in this graph that they did a deployment then Ph P warnings started spiking they did another deployment, and that took care of it.  Just a little bit of a problem left over, another deployment knocked it out.  So, catching that information that you can then use to graph in interesting ways and compare to lot of other different data sources is very important to very quickly isolating problems and getting them resolved production quickly.  There's excellent blog which I won't go into too much from digital ocean on setting up graphite, stats D and getting those all working together to basically catch this kind of information.  I have a little resources page at the end of this that I'll leave up so -- also when it's online.  Finally chatops this is somewhat of a -- came on around the whole time the continuous deployment was kicking off.  This is a term coined by GitHub in 2009.  The main problem they saw was GitHub was extremely distributed.  Beyond their core set of founders they were very, very distributed worldwide for their teams doing all of these deployments.  And they needed a way to get everybody in a room, so to speak, that was very geographically disbursed.  They determined that the best way to do that would be to automating through chat commands.  They wrote a chat bot called who bot, that is the logo for it, that basically they could issue commands to, and then who bot on the back end would go out and contact services via their APSs and kick off commands whether it's Jenkins or other systems.  How many people have heard the single pane of glass term?
  Companies I used to work for it became like a very, very derogatory term.  It was used very mockingly.  But as far as I've seen chatops is closest I've seen to single pane of glass in practice.  It's very cool to see that concept actually working.  So, some of the benefits of chatops everybody knows what everyone else is doing.  You see people in the chat room issuing commands and doing actions.  So, there's very high level of transparency across the teams.  A lot of times you'll also see things like Jenkins build status, even some logging output from other systems, they will feed those into other chat rooms that people can keep an eye on you don't have to hop around a whole bunch of different interfaces to get an idea of what is going on.  Just switch between chat rooms.  Because of this transparency, it's very easy for new users in your organization to come on board and see how things work.  Also these chat bots tip I cany have pretty involved help menus you can say, you know, chat bot help command, it will tell you how to issue a command.  People that are used to the CLI this is probably nothing really new or interesting.  It's something CLI people have known for a long time.  For ops people that may not have that desire to learn the CLI this really helps increase efficiency and communication.  Communication is again very, very instantaneous.  You no longer have all of these breaks in your communication where e-mail or calls are going on or something breaks and they say, okay, everybody get up and go into the war room we're going to figure out what is going on.  Then VP comes in and says, give me a status update right now.  Everybody is in the chat room including those VPs and highered up so they see what's going on and what -- like realtime status of what is happening for any given issue.  Finally time to execute actions is greatly reduced.  As long as it takes to type the command to do something and to kick it off.  You no longer have to hop to another systems interface and click on a button wait for that status then go to another system like, go to Nagios and disable things it's all pretty much automated for you on the back end just type a single command.  Some of the case studies I'm going to get into.  The firsts Atlassian is pretty well-known software company.  Apologize if that is small.  Atlassian" they bought the wiki and other data management system.  They use Ansible in their continuous deployment set up to tackle what they call the last mile problem, which is how they do that complex orchestration of pushing their build artifacts out to their system.  So from their blog that they go into, which well be in my references point at the end of this, their developers do everything on feature branches specific to them and they have their continuous integration which is bamboo.  Testing against each of the developers' branches when that is good to go they then merge that feature branch into their release branch, bamboo kicked off builds a build artifact which is a binary.  And they then use Ansible in each of their staging, QA and production environments to push that exact spatial binary to each one of those environments.  Ensuring that nothing sneaks in there, they know if their environments are the same the binary is the same, things should work from environment to environment.  Some of the advantages, promoting the same build it's consistent they know it.  Number two, their blog goes into -- definitely take time out to mention that just because they're doing this with the products they offer, their methodology is very interchangeable with just about any other tool that you could insert into any one of these slots.  You could use Jenkins instead of bamboo, use something besides Ansible if you really wanted to use something else.  The disadvantages in reading the description they're very siloed.  They have individual terms that have their turf for each environment.  It's not very DevOps-y.  The enterprise way of doing things which is working for them, but in my history I've seen it break down when you actually do run into problems at times.  So, RisingStack, this is they are doing code reviews for companies writing java script,  some other things.  They use CodeShip which is a third party kind of continuous integration as a service which obviously Travis kind of is, too.  And custom service which they call the docker deployer, that is something they wrote in house that basically just listens for a web hook call.  So from, their blog the way they describe it they have all of their Dev-L goes into the -- to CodeShip.  It does all of their testing and all their integration testing all their other testing and then if that passes they contact docker hub to build a new docker image.  Docker hub also supports web hooks on actions when this image is done building that web hook is fired to that in house service they wrote and that essentially just runs Ansible to deploy that new docker container out to their environment.  And starts the new once, stops the old ones, whatever they need to do.  They don't go into too much how they do it if it's very complex for them to push that container out.  Knowing docker I don't think so it's very complex.  Risingstack continued some of the advantages, completely automated.  They don't have any human intervention past that initial commit.  They rely on their testing and if there's a problem, it's pretty easy to just go ahead and do another fix or roll back to a previous version of that docker container.  Some of the disadvantages, obviously they wrote that in-house service.  So it's something they have to maintain when doing something like Jenkins or something else might have worked -- Jenkins or something else might have worked just as well.  Bigpanda, we love bigpanda.  They spoke at our last Ansible fest in San Francisco.  They are centered around automated incident management they basically correlate events and some other information and try to help you pin things down when you're having a problem.  They use chatops and Ansible very heavily in their day to day operations.  They typically say this they have operations team of one.  It's just one gentleman who basically manages everything but being a very small organization and having everything in chat everybody else can pretty much see what that one person is doing.  And they can kind of take over if he's unavailable to do something.  They wrote a very nice blog about us five reasons we love using Ansible for continuous delivery, if anybody wants to read that.  One of their photos that they bring up in their presentations is, "Take the hardest part, scariest part of our build and make it the easiest" which if that makes total sense, if something is hard to do, tackling that, get the big wins first so that you can then work on something that isn't quite as difficult to figure out.  When you figure out the hard things everything else is easy.  So, this is -- they use Hubot which I mentioned and hipchat, renamed Hubot to beanbot which is a panda driving a larger panda Mecca, fun little stuff that they do.  But they basically just very straight forward similar to Atlassian when you get down to it where you have Ansible in the back end doing all those deployments after your continuous integration system is done with the build artifact.  So, finally grasswire this is our kind of obligatory tower.  If you don't know about tower, tower is our enterprise product.  It offers couple advances like rest API that you can -- rest -- things talking to it directly.  They are also a big chat Opsman at grasswire they use slack instead  of hipchat.  One of the cool things about slack is that you can have it natively do web hooks from the interface.  You can do add-ons to it without having to have a full featured chat bot in your environment if you don't want to maintain that running some where.
  So, pretty much their workflow is they use packer to build new AMIs which they put into EC2.  They use TerraForm which is a nice command line program to do kind of complex EC2 environments if you're not happy with cloud formation and still want to fire everything off in one giant environment build, they say that they're able to do more complex networking set ups with it that way.  Once they have their environment up and running they use Ansible to deploy their build artifacts just like everybody else.  Use Ansible EC2 modules for the elastic load balancer to take things out of load balancer just like I said.  Stand up database systems.  What have you.  They did mention in their blog that they originally were baking their build artifacts into an Ami every time that they basically changed anything.  They found that to be a really, really heavy-handed approach to do things.  They ended up reverting that, splitting it into two parts that maybe something you consider probably less of a problem with docker, but when you're doing full featured VMs definitely something to think about.  So, one of the other kind of cool features they use for packer is, tower provides this provisioning callback system.  So instead of having to, in your docker or vagrant file or anything else you just make this callback to tower and say, come configure me.  And it will based on your -- host name that you ask for it will drop whatever configuration rolls what you want that system to do in your environment.  This is their provisioning little snippet from their packer file and just shows they drop a shell script into their RC.local that gets run when it's building.  Pretty easy set up.  Also since they're using tower, they're able to use the tower CLI command which is just a command like it says to go out and trigger jobs and tower so they don't have 20 write out Ansible playbook then the name of the playbook all these other things they have all of these discrete jobs set up.  Pretty simple set up.  So, some of the tips and tricks I picked up from reading all of these blogs and kind of going through seeing how companies were using us.  It's not too many, first and foremost use Ansible to build these environments, like I said.  There is tons of roles out there to basically install everyone of these components in any manner you so desire.  Whether it's bare metal in your datacenter or Amazon or Rackspace, wherever you want to build it.  It makes it just -- saves you a lot of time.  You can also if it's not on galaxy you can find it via Google or GitHub is -- very easy to find them.  Second and last tip just use Ansible from day one.  This is true of any configuration management system.  If it's worth your time to go out and SSH to a system it's worth the time to just write it into your playbook immediately then just run your playbook.  It will receive save you the time later of trying the figure out what you had to do to get something configured and inevitably forget to enable a service when you install something and something gets rebooted a hundred days later and nothing works when it comes back up.  That saves you the time and trouble of having to fix things down the road.  So, ready for Q&A session if anybody wants to come up to the microphone.
  >> If you have any questions.  People are already there.
  >> Audience: I have question about Ansible and docker.  So, you mentioned that Ansible okay work with docker so it can start and stop docker containers?
  >> Yes.  We have a docker module and also a docker image module for building docker images.  They could be used to start and stop containers, download things from docker hubs, whether private or local.
  >> Audience: Does that mean that we can use Ansible as a another kind of docker orchestration tool?
  >> Absolutely.  
  >> Cool, thanks.
  >> Audience: Lot of these folks are using lots of commercial services, GitHub, hipchat, docker hub, that's huge attack surface most of these are putting their crown jewels of their company in these services I'm wondering if there is -- you want to use these things because they're public and easy to use, but what are some of the ways that you can protect yourself against, say, compromise.  
  One of their services or authentication fails and someone gets into your chat room who shouldn't be there.
  >> There's a lot of ways you can tackle that.
  You can build some of the authorization into the chat bot that you're running.  There's a few other ways you can do it.   If you're using tower, tower has additional roll-based access controls.  It really depends on how far you want to take it instead of using GitHub you can install something locally in your environment like Git lab or Git repo if you're really paranoid about having your source code out.  There's a lot of options, people obviously take a lot of different routes with it.  If you're always paranoid you can always bring things inhouse, there's always open source projects that basically will provide you the same kind of functionality.  You can run internal IRC server, you can run like I said internal source control.  Just depends how far you want to go with it.  You always have to remember, too, that a lot of these third party services have incredible services teams, too.  And they're constantly fending off attacks and other things like that.  Like the DDOS against GitHub over last few weeks.
  >> Audience: For someone who would like to create a module can you recommend one module right now that is a good starting point to understand how Ansible modules work and one that is easy but works across different operating systems?
  >> Sure.  There's a couple that come to mind like some of the things and extras, like if you want to see how some integration with third party APIs works, you can look at the HipChat module, principle.  Some of the more basic ones that are like the core important ones like copy and service are actually really complex.  I wouldn't necessarily recommend looking at those.  There's a lot.  Talk to me afterwards we can discuss some.  There's a few that pop to mind.
  >> Audience: Another docker question.  Say I want to use Ansible to run one command on a number of docker containers, it's actually considered a better practice as far as I know to not use SSHD in docker container.  So, how would I -- would it be possible to do with Ansible?
  >> Yeah.  Ansible actually does have some at connection methods.  You can actually have -- if Ansible running local to the docker container, you can have Ansible connect over I believe either Libvert or some of the other methods to connect to the container not over SSHD.  They are still kind of new because not a lot of people have really done that yet.  But the option is out there.  If it needs improvement just let us know we can always work on it make it better.
  >> Thank you.
  >> Audience: So you talked about monitoring I'm curious, how does Ansible work with, say, the back end where does it turn it off automatically, some sort of module built into Ansible that you just pointed out the server or have to know the calls to the Nagio server if you were turning it off in the command line?
  >> There is a Nagios module it can be used to enable and reenable service name.  We have Ansible, delegated to action.  That means rather than running it  on like say you have this list of hosts, one through ten, rather than running it on any host 1-10 we say delegate to the Nagios server and Ansible connects only to that server and runs that module there.  So, yeah, just if you look at the document on the Nagios module you don't have to know any of the nitty-gritty details for it.
  >> Audience: Hi.  In my organization we're in the middle of implementing SaltStack.  And since coming to PyCon I've been hearing a lot of positive things about Ansible.  What would be the arguments or reasons to switch over to Ansible?
  >> Obviously we always tell people that ask us questions like this, try them all out find the one that works best for you.  We just typically find that people pick Ansible for those reasons of simplicity that I mentioned.  No required agent on your system is obviously a huge bonus.  SaltStack does offer an agentless mode, though correct me if I'm wrong last I heard still kind of recommended that you not use that for production set-ups.  Then beyond that I believe SaltStack is one of the ones I haven't used, actually.  I believe they still do things in a very declarative way, more along the lines of how puppet works where there's not a strict ordering.  Again correct me if I'm wrong I thought that was one of the other primary differences.  Yeah, obviously salt and Ansible share a lot of the same stack under the hood with Python and Ginga 2 a lot of the underlying technologies are very similar just we took slightly different approach to it.
  >> Audience: Cool, thank you, no problem.  All right.  I think that's it.  Thank you, James.
  [ Applause ]
  >> Thank you.  
